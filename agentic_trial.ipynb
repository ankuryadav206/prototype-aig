{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aac43454",
   "metadata": {},
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7635f44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Any, Optional, TypedDict\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# LangChain and LangGraph imports\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import BedrockEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "\n",
    "# LangGraph imports\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "# Additional utilities\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0ad58f",
   "metadata": {},
   "source": [
    "# Configuration and AWS Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec91367f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS Configuration\n",
    "AWS_CONFIG = {\n",
    "    \"region_name\": \"us-west-2\",\n",
    "    \"model_id\": \"anthropic.claude-3-5-sonnet-20241022-v2:0\"\n",
    "}\n",
    "\n",
    "# Initialize Bedrock client\n",
    "try:\n",
    "    bedrock_llm = ChatBedrock(\n",
    "        model_id=AWS_CONFIG[\"model_id\"],\n",
    "        region_name=AWS_CONFIG[\"region_name\"],\n",
    "        model_kwargs={\n",
    "            \"max_tokens\": 4000,\n",
    "            \"temperature\": 0.1\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    bedrock_embeddings = BedrockEmbeddings(\n",
    "        model_id=\"amazon.titan-embed-text-v1\",\n",
    "        region_name=AWS_CONFIG[\"region_name\"]\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ AWS Bedrock client initialized successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error initializing Bedrock: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66837203",
   "metadata": {},
   "source": [
    "# State Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cbac48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    \"\"\"State for the agent workflow\"\"\"\n",
    "    messages: List[str]\n",
    "    user_input: str\n",
    "    current_step: str\n",
    "    tool_calls: List[str]\n",
    "    retrieved_info: Dict[str, Any]\n",
    "    final_response: str\n",
    "    needs_more_tools: bool\n",
    "    conversation_history: List[Dict[str, str]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fdacc6",
   "metadata": {},
   "source": [
    "# Data Loading and Vector Database Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c90143",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorDatabaseManager:\n",
    "    \"\"\"Manages FAISS vector databases for different retrievers\"\"\"\n",
    "    \n",
    "    def __init__(self, embeddings, persist_directory=\"./vector_stores\"):\n",
    "        self.embeddings = embeddings\n",
    "        self.persist_directory = Path(persist_directory)\n",
    "        self.persist_directory.mkdir(exist_ok=True)\n",
    "        self.vector_stores = {}\n",
    "        self.retrievers = {}\n",
    "        \n",
    "    def create_vector_store_from_excel(self, excel_path: str, store_name: str, \n",
    "                                     text_columns: List[str], metadata_columns: List[str] = None):\n",
    "        \"\"\"Create FAISS vector store from Excel file\"\"\"\n",
    "        try:\n",
    "            # Read Excel file\n",
    "            df = pd.read_excel(excel_path)\n",
    "            print(f\"üìä Loaded {len(df)} records from {excel_path}\")\n",
    "            \n",
    "            # Create documents\n",
    "            documents = []\n",
    "            for idx, row in df.iterrows():\n",
    "                # Combine text columns\n",
    "                text_content = \" | \".join([str(row[col]) for col in text_columns if pd.notna(row[col])])\n",
    "                \n",
    "                # Create metadata\n",
    "                metadata = {\"source\": excel_path, \"row_id\": idx}\n",
    "                if metadata_columns:\n",
    "                    for col in metadata_columns:\n",
    "                        if col in df.columns and pd.notna(row[col]):\n",
    "                            metadata[col] = str(row[col])\n",
    "                \n",
    "                documents.append(Document(page_content=text_content, metadata=metadata))\n",
    "            \n",
    "            # Split documents if needed\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=1000,\n",
    "                chunk_overlap=200\n",
    "            )\n",
    "            split_docs = text_splitter.split_documents(documents)\n",
    "            \n",
    "            # Create FAISS vector store\n",
    "            vector_store = FAISS.from_documents(split_docs, self.embeddings)\n",
    "            \n",
    "            # Save to disk\n",
    "            store_path = self.persist_directory / store_name\n",
    "            vector_store.save_local(str(store_path))\n",
    "            \n",
    "            self.vector_stores[store_name] = vector_store\n",
    "            print(f\"‚úÖ Created and saved vector store '{store_name}' with {len(split_docs)} documents\")\n",
    "            \n",
    "            return vector_store\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error creating vector store {store_name}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def load_vector_store(self, store_name: str):\n",
    "        \"\"\"Load existing vector store from disk\"\"\"\n",
    "        try:\n",
    "            store_path = self.persist_directory / store_name\n",
    "            if store_path.exists():\n",
    "                vector_store = FAISS.load_local(str(store_path), self.embeddings, allow_dangerous_deserialization=True)\n",
    "                self.vector_stores[store_name] = vector_store\n",
    "                print(f\"‚úÖ Loaded existing vector store: {store_name}\")\n",
    "                return vector_store\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Vector store {store_name} not found at {store_path}\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading vector store {store_name}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def create_self_query_retriever(self, store_name: str, document_content_description: str, \n",
    "                                  metadata_field_info: List[AttributeInfo]):\n",
    "        \"\"\"Create self-query retriever for a vector store\"\"\"\n",
    "        try:\n",
    "            if store_name not in self.vector_stores:\n",
    "                print(f\"‚ùå Vector store {store_name} not found\")\n",
    "                return None\n",
    "            \n",
    "            retriever = SelfQueryRetriever.from_llm(\n",
    "                llm=bedrock_llm,\n",
    "                vectorstore=self.vector_stores[store_name],\n",
    "                document_contents=document_content_description,\n",
    "                metadata_field_info=metadata_field_info,\n",
    "                verbose=True\n",
    "            )\n",
    "            \n",
    "            self.retrievers[store_name] = retriever\n",
    "            print(f\"‚úÖ Created self-query retriever for {store_name}\")\n",
    "            return retriever\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error creating self-query retriever for {store_name}: {e}\")\n",
    "            # Fallback to regular retriever\n",
    "            retriever = self.vector_stores[store_name].as_retriever(search_kwargs={\"k\": 5})\n",
    "            self.retrievers[store_name] = retriever\n",
    "            print(f\"‚úÖ Created fallback retriever for {store_name}\")\n",
    "            return retriever\n",
    "\n",
    "# Initialize Vector Database Manager\n",
    "vector_db_manager = VectorDatabaseManager(bedrock_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0cbccd",
   "metadata": {},
   "source": [
    "# Sample Data Creation (Replace with your Excel files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7badcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample Excel files for demonstration\n",
    "# Replace this section with your actual Excel file paths\n",
    "\n",
    "def create_sample_data():\n",
    "    \"\"\"Create sample Excel files for testing\"\"\"\n",
    "    \n",
    "    # Sample Change Management data\n",
    "    change_data = {\n",
    "        'change_id': ['CHG001', 'CHG002', 'CHG003'],\n",
    "        'description': ['Server upgrade maintenance', 'Database migration', 'Network configuration update'],\n",
    "        'status': ['Completed', 'In Progress', 'Planned'],\n",
    "        'priority': ['High', 'Medium', 'Low'],\n",
    "        'assigned_to': ['John Doe', 'Jane Smith', 'Bob Johnson']\n",
    "    }\n",
    "    pd.DataFrame(change_data).to_excel('./sample_change_data.xlsx', index=False)\n",
    "    \n",
    "    # Sample CMDB data\n",
    "    cmdb_data = {\n",
    "        'ci_id': ['CI001', 'CI002', 'CI003'],\n",
    "        'ci_name': ['Production Server 1', 'Database Server', 'Load Balancer'],\n",
    "        'ci_type': ['Server', 'Database', 'Network Device'],\n",
    "        'status': ['Active', 'Active', 'Maintenance'],\n",
    "        'location': ['Data Center A', 'Data Center B', 'Data Center A']\n",
    "    }\n",
    "    pd.DataFrame(cmdb_data).to_excel('./sample_cmdb_data.xlsx', index=False)\n",
    "    \n",
    "    # Sample Incident data\n",
    "    incident_data = {\n",
    "        'incident_id': ['INC001', 'INC002', 'INC003'],\n",
    "        'description': ['Server down', 'Database connection issues', 'Network latency problems'],\n",
    "        'status': ['Resolved', 'Open', 'In Progress'],\n",
    "        'priority': ['Critical', 'High', 'Medium'],\n",
    "        'assigned_to': ['Support Team A', 'DBA Team', 'Network Team']\n",
    "    }\n",
    "    pd.DataFrame(incident_data).to_excel('./sample_incident_data.xlsx', index=False)\n",
    "    \n",
    "    print(\"‚úÖ Sample data files created!\")\n",
    "\n",
    "# Create sample data (remove this if you have actual Excel files)\n",
    "create_sample_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa9dfeb",
   "metadata": {},
   "source": [
    "# Vector Store Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04977017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or load vector stores for each retriever\n",
    "print(\"üîß Setting up vector stores...\")\n",
    "\n",
    "# Change Management Vector Store\n",
    "change_vs = vector_db_manager.load_vector_store(\"change_management\")\n",
    "if not change_vs:\n",
    "    change_vs = vector_db_manager.create_vector_store_from_excel(\n",
    "        \"./sample_change_data.xlsx\",\n",
    "        \"change_management\",\n",
    "        text_columns=['description', 'status', 'priority'],\n",
    "        metadata_columns=['change_id', 'assigned_to']\n",
    "    )\n",
    "\n",
    "# CMDB Vector Store\n",
    "cmdb_vs = vector_db_manager.load_vector_store(\"cmdb\")\n",
    "if not cmdb_vs:\n",
    "    cmdb_vs = vector_db_manager.create_vector_store_from_excel(\n",
    "        \"./sample_cmdb_data.xlsx\",\n",
    "        \"cmdb\",\n",
    "        text_columns=['ci_name', 'ci_type', 'status'],\n",
    "        metadata_columns=['ci_id', 'location']\n",
    "    )\n",
    "\n",
    "# Incident Vector Store\n",
    "incident_vs = vector_db_manager.load_vector_store(\"incident\")\n",
    "if not incident_vs:\n",
    "    incident_vs = vector_db_manager.create_vector_store_from_excel(\n",
    "        \"./sample_incident_data.xlsx\",\n",
    "        \"incident\",\n",
    "        text_columns=['description', 'status', 'priority'],\n",
    "        metadata_columns=['incident_id', 'assigned_to']\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ All vector stores ready!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd0af65",
   "metadata": {},
   "source": [
    "# Self-Query Retrievers Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bf3b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metadata field information for self-query retrievers\n",
    "\n",
    "# Change Management metadata\n",
    "change_metadata_fields = [\n",
    "    AttributeInfo(name=\"change_id\", description=\"Unique change identifier\", type=\"string\"),\n",
    "    AttributeInfo(name=\"assigned_to\", description=\"Person assigned to the change\", type=\"string\"),\n",
    "    AttributeInfo(name=\"source\", description=\"Source of the document\", type=\"string\")\n",
    "]\n",
    "\n",
    "# CMDB metadata\n",
    "cmdb_metadata_fields = [\n",
    "    AttributeInfo(name=\"ci_id\", description=\"Configuration Item identifier\", type=\"string\"),\n",
    "    AttributeInfo(name=\"location\", description=\"Physical location of the CI\", type=\"string\"),\n",
    "    AttributeInfo(name=\"source\", description=\"Source of the document\", type=\"string\")\n",
    "]\n",
    "\n",
    "# Incident metadata\n",
    "incident_metadata_fields = [\n",
    "    AttributeInfo(name=\"incident_id\", description=\"Unique incident identifier\", type=\"string\"),\n",
    "    AttributeInfo(name=\"assigned_to\", description=\"Team assigned to the incident\", type=\"string\"),\n",
    "    AttributeInfo(name=\"source\", description=\"Source of the document\", type=\"string\")\n",
    "]\n",
    "\n",
    "# Create self-query retrievers\n",
    "change_retriever = vector_db_manager.create_self_query_retriever(\n",
    "    \"change_management\",\n",
    "    \"Change management records including descriptions, status, and priority information\",\n",
    "    change_metadata_fields\n",
    ")\n",
    "\n",
    "cmdb_retriever = vector_db_manager.create_self_query_retriever(\n",
    "    \"cmdb\",\n",
    "    \"Configuration Management Database records with CI information\",\n",
    "    cmdb_metadata_fields\n",
    ")\n",
    "\n",
    "incident_retriever = vector_db_manager.create_self_query_retriever(\n",
    "    \"incident\",\n",
    "    \"Incident management records with descriptions and status information\",\n",
    "    incident_metadata_fields\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Self-query retrievers created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af064d2",
   "metadata": {},
   "source": [
    "# Tool Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80511944",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatbotTools:\n",
    "    \"\"\"Collection of tools for the chatbot\"\"\"\n",
    "    \n",
    "    def __init__(self, llm, retrievers):\n",
    "        self.llm = llm\n",
    "        self.retrievers = retrievers\n",
    "        self.available_prompts = {\n",
    "            \"change_queries\": \"Ask about change management, deployments, or maintenance activities\",\n",
    "            \"cmdb_queries\": \"Ask about configuration items, servers, databases, or infrastructure\",\n",
    "            \"incident_queries\": \"Ask about incidents, outages, or technical issues\",\n",
    "            \"general_questions\": \"Ask general questions for AI assistance\"\n",
    "        }\n",
    "    \n",
    "    def greeting_tool(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Handle greetings and show available prompts\"\"\"\n",
    "        greeting_response = f\"\"\"\n",
    "        Hello! üëã Welcome to the IT Service Management Assistant!\n",
    "        \n",
    "        I can help you with the following:\n",
    "        \n",
    "        üîß **Change Management**: {self.available_prompts['change_queries']}\n",
    "        üñ•Ô∏è  **CMDB Queries**: {self.available_prompts['cmdb_queries']}\n",
    "        üö® **Incident Management**: {self.available_prompts['incident_queries']}\n",
    "        ü§ñ **General Questions**: {self.available_prompts['general_questions']}\n",
    "        \n",
    "        What would you like to know about today?\n",
    "        \"\"\"\n",
    "        \n",
    "        state[\"retrieved_info\"][\"greeting\"] = greeting_response\n",
    "        state[\"tool_calls\"].append(\"greeting\")\n",
    "        return state\n",
    "    \n",
    "    def change_retriever_tool(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Retrieve change management information\"\"\"\n",
    "        try:\n",
    "            query = state[\"user_input\"]\n",
    "            docs = self.retrievers[\"change_management\"].invoke(query)\n",
    "            \n",
    "            change_info = []\n",
    "            for doc in docs:\n",
    "                change_info.append({\n",
    "                    \"content\": doc.page_content,\n",
    "                    \"metadata\": doc.metadata\n",
    "                })\n",
    "            \n",
    "            # Generate response based on retrieved information\n",
    "            context = \"\\n\".join([doc[\"content\"] for doc in change_info])\n",
    "            response = self.llm.invoke(f\"\"\"\n",
    "            Based on the following change management information, please provide a helpful response to the user query: \"{query}\"\n",
    "            \n",
    "            Change Management Data:\n",
    "            {context}\n",
    "            \n",
    "            Please provide a clear and informative response.\n",
    "            \"\"\").content\n",
    "            \n",
    "            state[\"retrieved_info\"][\"change_management\"] = {\n",
    "                \"response\": response,\n",
    "                \"docs\": change_info\n",
    "            }\n",
    "            state[\"tool_calls\"].append(\"change_retriever\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            state[\"retrieved_info\"][\"change_management\"] = {\n",
    "                \"response\": f\"Sorry, I encountered an error retrieving change management information: {str(e)}\",\n",
    "                \"docs\": []\n",
    "            }\n",
    "            state[\"tool_calls\"].append(\"change_retriever_error\")\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def cmdb_retriever_tool(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Retrieve CMDB information\"\"\"\n",
    "        try:\n",
    "            query = state[\"user_input\"]\n",
    "            docs = self.retrievers[\"cmdb\"].invoke(query)\n",
    "            \n",
    "            cmdb_info = []\n",
    "            for doc in docs:\n",
    "                cmdb_info.append({\n",
    "                    \"content\": doc.page_content,\n",
    "                    \"metadata\": doc.metadata\n",
    "                })\n",
    "            \n",
    "            context = \"\\n\".join([doc[\"content\"] for doc in cmdb_info])\n",
    "            response = self.llm.invoke(f\"\"\"\n",
    "            Based on the following CMDB information, please provide a helpful response to the user query: \"{query}\"\n",
    "            \n",
    "            CMDB Data:\n",
    "            {context}\n",
    "            \n",
    "            Please provide a clear and informative response about the configuration items.\n",
    "            \"\"\").content\n",
    "            \n",
    "            state[\"retrieved_info\"][\"cmdb\"] = {\n",
    "                \"response\": response,\n",
    "                \"docs\": cmdb_info\n",
    "            }\n",
    "            state[\"tool_calls\"].append(\"cmdb_retriever\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            state[\"retrieved_info\"][\"cmdb\"] = {\n",
    "                \"response\": f\"Sorry, I encountered an error retrieving CMDB information: {str(e)}\",\n",
    "                \"docs\": []\n",
    "            }\n",
    "            state[\"tool_calls\"].append(\"cmdb_retriever_error\")\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def incident_retriever_tool(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Retrieve incident information\"\"\"\n",
    "        try:\n",
    "            query = state[\"user_input\"]\n",
    "            docs = self.retrievers[\"incident\"].invoke(query)\n",
    "            \n",
    "            incident_info = []\n",
    "            for doc in docs:\n",
    "                incident_info.append({\n",
    "                    \"content\": doc.page_content,\n",
    "                    \"metadata\": doc.metadata\n",
    "                })\n",
    "            \n",
    "            context = \"\\n\".join([doc[\"content\"] for doc in incident_info])\n",
    "            response = self.llm.invoke(f\"\"\"\n",
    "            Based on the following incident information, please provide a helpful response to the user query: \"{query}\"\n",
    "            \n",
    "            Incident Data:\n",
    "            {context}\n",
    "            \n",
    "            Please provide a clear and informative response about the incidents.\n",
    "            \"\"\").content\n",
    "            \n",
    "            state[\"retrieved_info\"][\"incidents\"] = {\n",
    "                \"response\": response,\n",
    "                \"docs\": incident_info\n",
    "            }\n",
    "            state[\"tool_calls\"].append(\"incident_retriever\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            state[\"retrieved_info\"][\"incidents\"] = {\n",
    "                \"response\": f\"Sorry, I encountered an error retrieving incident information: {str(e)}\",\n",
    "                \"docs\": []\n",
    "            }\n",
    "            state[\"tool_calls\"].append(\"incident_retriever_error\")\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def basic_llm_tool(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Handle general queries with basic LLM response\"\"\"\n",
    "        try:\n",
    "            query = state[\"user_input\"]\n",
    "            response = self.llm.invoke(f\"\"\"\n",
    "            Please provide a helpful response to this general question: \"{query}\"\n",
    "            \n",
    "            Keep your response informative and friendly. If this seems like it should be handled by a specific IT service management tool, \n",
    "            suggest that the user ask about change management, CMDB, or incident management specifically.\n",
    "            \"\"\").content\n",
    "            \n",
    "            state[\"retrieved_info\"][\"general\"] = response\n",
    "            state[\"tool_calls\"].append(\"basic_llm\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            state[\"retrieved_info\"][\"general\"] = f\"Sorry, I encountered an error: {str(e)}\"\n",
    "            state[\"tool_calls\"].append(\"basic_llm_error\")\n",
    "        \n",
    "        return state\n",
    "\n",
    "# Initialize tools\n",
    "retrievers_dict = {\n",
    "    \"change_management\": change_retriever,\n",
    "    \"cmdb\": cmdb_retriever,\n",
    "    \"incident\": incident_retriever\n",
    "}\n",
    "\n",
    "chatbot_tools = ChatbotTools(bedrock_llm, retrievers_dict)\n",
    "print(\"‚úÖ Chatbot tools initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99a5d30",
   "metadata": {},
   "source": [
    "# Router Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a6c229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def router_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Router to classify user input and determine which tools to call\"\"\"\n",
    "    \n",
    "    user_input = state[\"user_input\"].lower()\n",
    "    state[\"current_step\"] = \"routing\"\n",
    "    \n",
    "    # Classification prompt\n",
    "    classification_prompt = f\"\"\"\n",
    "    Classify the following user input and determine which tools should be called. \n",
    "    The user input is: \"{state['user_input']}\"\n",
    "    \n",
    "    Available tools:\n",
    "    1. greeting - for greetings like \"hi\", \"hello\", \"help\"\n",
    "    2. change_retriever - for change management, deployment, maintenance queries\n",
    "    3. cmdb_retriever - for configuration items, infrastructure, server queries  \n",
    "    4. incident_retriever - for incident, outage, problem queries\n",
    "    5. basic_llm - for general questions not related to specific IT services\n",
    "    \n",
    "    You can select multiple tools if the query is complex and needs information from multiple sources.\n",
    "    \n",
    "    Respond with a JSON object with the following structure:\n",
    "    {{\n",
    "        \"tools_needed\": [\"tool1\", \"tool2\", ...],\n",
    "        \"reasoning\": \"explanation of why these tools were selected\"\n",
    "    }}\n",
    "    \n",
    "    Examples:\n",
    "    - \"Hello\" -> {{\"tools_needed\": [\"greeting\"], \"reasoning\": \"User is greeting\"}}\n",
    "    - \"What changes are in progress?\" -> {{\"tools_needed\": [\"change_retriever\"], \"reasoning\": \"User asking about change management\"}}\n",
    "    - \"Show me server CI001 and any related incidents\" -> {{\"tools_needed\": [\"cmdb_retriever\", \"incident_retriever\"], \"reasoning\": \"User needs both CMDB info and incident information\"}}\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        classification_response = bedrock_llm.invoke(classification_prompt).content\n",
    "        \n",
    "        # Try to parse JSON response\n",
    "        try:\n",
    "            classification_data = json.loads(classification_response)\n",
    "            tools_needed = classification_data.get(\"tools_needed\", [\"basic_llm\"])\n",
    "        except json.JSONDecodeError:\n",
    "            # Fallback classification based on keywords\n",
    "            tools_needed = []\n",
    "            \n",
    "            if any(greeting in user_input for greeting in [\"hi\", \"hello\", \"help\", \"greeting\"]):\n",
    "                tools_needed.append(\"greeting\")\n",
    "            elif any(word in user_input for word in [\"change\", \"deployment\", \"maintenance\", \"chg\"]):\n",
    "                tools_needed.append(\"change_retriever\")\n",
    "            elif any(word in user_input for word in [\"cmdb\", \"configuration\", \"server\", \"ci\", \"infrastructure\"]):\n",
    "                tools_needed.append(\"cmdb_retriever\")\n",
    "            elif any(word in user_input for word in [\"incident\", \"outage\", \"problem\", \"inc\", \"issue\"]):\n",
    "                tools_needed.append(\"incident_retriever\")\n",
    "            else:\n",
    "                tools_needed.append(\"basic_llm\")\n",
    "        \n",
    "        state[\"messages\"].append(f\"Router selected tools: {tools_needed}\")\n",
    "        \n",
    "        # Execute selected tools\n",
    "        for tool in tools_needed:\n",
    "            if tool == \"greeting\":\n",
    "                state = chatbot_tools.greeting_tool(state)\n",
    "            elif tool == \"change_retriever\":\n",
    "                state = chatbot_tools.change_retriever_tool(state)\n",
    "            elif tool == \"cmdb_retriever\":\n",
    "                state = chatbot_tools.cmdb_retriever_tool(state)\n",
    "            elif tool == \"incident_retriever\":\n",
    "                state = chatbot_tools.incident_retriever_tool(state)\n",
    "            elif tool == \"basic_llm\":\n",
    "                state = chatbot_tools.basic_llm_tool(state)\n",
    "        \n",
    "        state[\"current_step\"] = \"tool_execution_complete\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        state[\"messages\"].append(f\"Router error: {str(e)}\")\n",
    "        state[\"current_step\"] = \"router_error\"\n",
    "        # Fallback to basic LLM\n",
    "        state = chatbot_tools.basic_llm_tool(state)\n",
    "    \n",
    "    return state\n",
    "\n",
    "print(\"‚úÖ Router implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa96abe",
   "metadata": {},
   "source": [
    "# Judge/Validator Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01dd690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def judge_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Judge to validate if the response is adequate or needs more tools\"\"\"\n",
    "    \n",
    "    state[\"current_step\"] = \"judging\"\n",
    "    \n",
    "    # Prepare context from all retrieved information\n",
    "    context_parts = []\n",
    "    for key, value in state[\"retrieved_info\"].items():\n",
    "        if isinstance(value, dict) and \"response\" in value:\n",
    "            context_parts.append(f\"{key}: {value['response']}\")\n",
    "        elif isinstance(value, str):\n",
    "            context_parts.append(f\"{key}: {value}\")\n",
    "    \n",
    "    combined_context = \"\\n\".join(context_parts)\n",
    "    \n",
    "    # Judge prompt\n",
    "    judge_prompt = f\"\"\"\n",
    "    Evaluate if the following information adequately answers the user's question: \"{state['user_input']}\"\n",
    "    \n",
    "    Retrieved Information:\n",
    "    {combined_context}\n",
    "    \n",
    "    Tools called: {state['tool_calls']}\n",
    "    \n",
    "    Please evaluate:\n",
    "    1. Does the information adequately answer the user's question?\n",
    "    2. Are there any gaps that would require additional tool calls?\n",
    "    3. Is the information relevant and helpful?\n",
    "    \n",
    "    Respond with a JSON object:\n",
    "    {{\n",
    "        \"is_adequate\": true/false,\n",
    "        \"reasoning\": \"explanation\",\n",
    "        \"suggested_additional_tools\": [\"tool1\", \"tool2\"] or [],\n",
    "        \"confidence_score\": 0.0-1.0\n",
    "    }}\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        judge_response = bedrock_llm.invoke(judge_prompt).content\n",
    "        \n",
    "        try:\n",
    "            judge_data = json.loads(judge_response)\n",
    "            is_adequate = judge_data.get(\"is_adequate\", True)\n",
    "            confidence = judge_data.get(\"confidence_score\", 0.8)\n",
    "        except json.JSONDecodeError:\n",
    "            # Fallback: assume adequate if we have some retrieved info\n",
    "            is_adequate = len(state[\"retrieved_info\"]) > 0\n",
    "            confidence = 0.7\n",
    "        \n",
    "        if is_adequate and confidence > 0.6:\n",
    "            state[\"needs_more_tools\"] = False\n",
    "            state[\"current_step\"] = \"creating_final_response\"\n",
    "        else:\n",
    "            state[\"needs_more_tools\"] = True\n",
    "            state[\"current_step\"] = \"needs_improvement\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        state[\"messages\"].append(f\"Judge error: {str(e)}\")\n",
    "        state[\"needs_more_tools\"] = False\n",
    "        state[\"current_step\"] = \"judge_error\"\n",
    "    \n",
    "    return state\n",
    "\n",
    "print(\"‚úÖ Judge/Validator implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55441403",
   "metadata": {},
   "source": [
    "# Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9726cae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_final_response(state: AgentState) -> AgentState:\n",
    "    \"\"\"Create the final consolidated response\"\"\"\n",
    "    \n",
    "    state[\"current_step\"] = \"generating_response\"\n",
    "    \n",
    "    if state[\"needs_more_tools\"]:\n",
    "        # If judge determined response is inadequate\n",
    "        fallback_response = \"\"\"\n",
    "        I've gathered some information for you, but I feel it might not be complete. \n",
    "        For now, here's what I found. We are continuously working on improving the system to provide better responses.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # Add available information\n",
    "        info_parts = []\n",
    "        for key, value in state[\"retrieved_info\"].items():\n",
    "            if isinstance(value, dict) and \"response\" in value:\n",
    "                info_parts.append(value[\"response\"])\n",
    "            elif isinstance(value, str):\n",
    "                info_parts.append(value)\n",
    "        \n",
    "        state[\"final_response\"] = fallback_response + \"\\n\".join(info_parts)\n",
    "    \n",
    "    else:\n",
    "        # Create comprehensive response from all retrieved information\n",
    "        if len(state[\"retrieved_info\"]) == 1:\n",
    "            # Single tool response\n",
    "            key, value = list(state[\"retrieved_info\"].items())[0]\n",
    "            if isinstance(value, dict) and \"response\" in value:\n",
    "                state[\"final_response\"] = value[\"response\"]\n",
    "            else:\n",
    "                state[\"final_response\"] = str(value)\n",
    "        \n",
    "        else:\n",
    "            # Multiple tool responses - consolidate\n",
    "            consolidation_prompt = f\"\"\"\n",
    "            The user asked: \"{state['user_input']}\"\n",
    "            \n",
    "            I have gathered information from multiple sources. Please create a comprehensive, \n",
    "            well-structured response that consolidates this information:\n",
    "            \n",
    "            \"\"\"\n",
    "            \n",
    "            for key, value in state[\"retrieved_info\"].items():\n",
    "                if isinstance(value, dict) and \"response\" in value:\n",
    "                    consolidation_prompt += f\"\\n{key.upper()}:\\n{value['response']}\\n\"\n",
    "                elif isinstance(value, str):\n",
    "                    consolidation_prompt += f\"\\n{key.upper()}:\\n{value}\\n\"\n",
    "            \n",
    "            consolidation_prompt += \"\"\"\n",
    "            \n",
    "            Please provide a clear, comprehensive response that addresses the user's question \n",
    "            using all relevant information above. Structure it in a user-friendly way.\n",
    "            \"\"\"\n",
    "            \n",
    "            try:\n",
    "                consolidated = bedrock_llm.invoke(consolidation_prompt).content\n",
    "                state[\"final_response\"] = consolidated\n",
    "            except Exception as e:\n",
    "                # Fallback to simple concatenation\n",
    "                response_parts = []\n",
    "                for key, value in state[\"retrieved_info\"].items():\n",
    "                    if isinstance(value, dict) and \"response\" in value:\n",
    "                        response_parts.append(f\"**{key.title()}**: {value['response']}\")\n",
    "                    elif isinstance(value, str):\n",
    "                        response_parts.append(f\"**{key.title()}**: {value}\")\n",
    "                \n",
    "                state[\"final_response\"] = \"\\n\\n\".join(response_parts)\n",
    "    \n",
    "    # Add to conversation history\n",
    "    state[\"conversation_history\"].append({\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"user_input\": state[\"user_input\"],\n",
    "        \"response\": state[\"final_response\"],\n",
    "        \"tools_used\": state[\"tool_calls\"]\n",
    "    })\n",
    "    \n",
    "    state[\"current_step\"] = \"complete\"\n",
    "    return state\n",
    "\n",
    "print(\"‚úÖ Response generation implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea81204",
   "metadata": {},
   "source": [
    "# Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b06b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Initialize the conversation state\"\"\"\n",
    "    if not state.get(\"conversation_history\"):\n",
    "        state[\"conversation_history\"] = []\n",
    "    \n",
    "    state[\"messages\"] = [f\"Processing user input: {state['user_input']}\"]\n",
    "    state[\"current_step\"] = \"started\"\n",
    "    state[\"tool_calls\"] = []\n",
    "    state[\"retrieved_info\"] = {}\n",
    "    state[\"needs_more_tools\"] = False\n",
    "    state[\"final_response\"] = \"\"\n",
    "    \n",
    "    return state\n",
    "\n",
    "# Create the state graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"start\", start_node)\n",
    "workflow.add_node(\"router\", router_node)\n",
    "workflow.add_node(\"judge\", judge_node)\n",
    "workflow.add_node(\"create_response\", create_final_response)\n",
    "\n",
    "# Add edges\n",
    "workflow.add_edge(START, \"start\")\n",
    "workflow.add_edge(\"start\", \"router\")\n",
    "workflow.add_edge(\"router\", \"judge\")\n",
    "workflow.add_edge(\"judge\", \"create_response\")\n",
    "workflow.add_edge(\"create_response\", END)\n",
    "\n",
    "# Set up memory\n",
    "memory = MemorySaver()\n",
    "\n",
    "# Compile the graph\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n",
    "print(\"‚úÖ LangGraph workflow created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80d3251",
   "metadata": {},
   "source": [
    "# Graph Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e800680",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from IPython.display import Image, display\n",
    "    import base64\n",
    "    from io import BytesIO\n",
    "    \n",
    "    # Get the graph visualization\n",
    "    graph_image = app.get_graph().draw_mermaid_png()\n",
    "    \n",
    "    # Display the graph\n",
    "    display(Image(graph_image))\n",
    "    print(\"‚úÖ Graph visualization displayed!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not display graph visualization: {e}\")\n",
    "    print(\"Graph structure:\")\n",
    "    print(\"START -> start -> router -> judge -> create_response -> END\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fbfc9a",
   "metadata": {},
   "source": [
    "# Chat Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4e4751",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatInterface:\n",
    "    \"\"\"Interactive chat interface for the agent\"\"\"\n",
    "    \n",
    "    def __init__(self, app, session_id=\"default_session\"):\n",
    "        self.app = app\n",
    "        self.session_id = session_id\n",
    "        self.config = {\"configurable\": {\"thread_id\": session_id}}\n",
    "    \n",
    "    def chat(self, user_input: str) -> str:\n",
    "        \"\"\"Process user input and return response\"\"\"\n",
    "        \n",
    "        # Create initial state\n",
    "        initial_state = {\n",
    "            \"user_input\": user_input,\n",
    "            \"messages\": [],\n",
    "            \"current_step\": \"\",\n",
    "            \"tool_calls\": [],\n",
    "            \"retrieved_info\": {},\n",
    "            \"final_response\": \"\",\n",
    "            \"needs_more_tools\": False,\n",
    "            \"conversation_history\": []\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Run the workflow\n",
    "            result = self.app.invoke(initial_state, config=self.config)\n",
    "            return result[\"final_response\"]\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Sorry, I encountered an error: {str(e)}\"\n",
    "    \n",
    "    def get_conversation_history(self):\n",
    "        \"\"\"Retrieve conversation history from memory\"\"\"\n",
    "        try:\n",
    "            # Get the latest state from memory\n",
    "            state = self.app.get_state(config=self.config)\n",
    "            if state and state.values and \"conversation_history\" in state.values:\n",
    "                return state.values[\"conversation_history\"]\n",
    "            return []\n",
    "        except:\n",
    "            return []\n",
    "    \n",
    "    def clear_history(self):\n",
    "        \"\"\"Clear conversation history\"\"\"\n",
    "        try:\n",
    "            # This would clear the thread - implementation depends on MemorySaver specifics\n",
    "            self.session_id = f\"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "            self.config = {\"configurable\": {\"thread_id\": self.session_id}}\n",
    "            print(\"‚úÖ Conversation history cleared!\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error clearing history: {e}\")\n",
    "\n",
    "# Initialize chat interface\n",
    "chat_interface = ChatInterface(app)\n",
    "print(\"‚úÖ Chat interface ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5979f4f7",
   "metadata": {},
   "source": [
    "# Example Usage and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7539359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_chatbot():\n",
    "    \"\"\"Test the chatbot with various queries\"\"\"\n",
    "    \n",
    "    test_queries = [\n",
    "        \"Hi there!\",\n",
    "        \"What changes are currently in progress?\",\n",
    "        \"Tell me about server CI001\",\n",
    "        \"Any critical incidents?\",\n",
    "        \"Show me information about database changes and any related incidents\",\n",
    "        \"What is artificial intelligence?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"üß™ Testing Chatbot with Sample Queries\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, query in enumerate(test_queries, 1):\n",
    "        print(f\"\\n{i}. User: {query}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        response = chat_interface.chat(query)\n",
    "        print(f\"Bot: {response}\")\n",
    "        \n",
    "        print(\"-\" * 30)\n",
    "\n",
    "# Run tests\n",
    "test_chatbot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dced5e73",
   "metadata": {},
   "source": [
    "# Interactive Chat Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda68160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_chat():\n",
    "    \"\"\"Interactive chat loop for real-time conversation\"\"\"\n",
    "    \n",
    "    print(\"\\nü§ñ IT Service Management Assistant\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Type 'quit', 'exit', or 'bye' to end the conversation\")\n",
    "    print(\"Type 'history' to see conversation history\")\n",
    "    print(\"Type 'clear' to clear conversation history\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            user_input = input(\"\\nüë§ You: \").strip()\n",
    "            \n",
    "            if user_input.lower() in ['quit', 'exit', 'bye']:\n",
    "                print(\"üëã Goodbye! Have a great day!\")\n",
    "                break\n",
    "            \n",
    "            elif user_input.lower() == 'history':\n",
    "                history = chat_interface.get_conversation_history()\n",
    "                if history:\n",
    "                    print(\"\\nüìö Conversation History:\")\n",
    "                    for i, entry in enumerate(history[-5:], 1):  # Show last 5\n",
    "                        print(f\"{i}. {entry['timestamp'][:19]}\")\n",
    "                        print(f\"   You: {entry['user_input']}\")\n",
    "                        print(f\"   Bot: {entry['response'][:100]}...\")\n",
    "                        print(f\"   Tools used: {', '.join(entry['tools_used'])}\")\n",
    "                else:\n",
    "                    print(\"No conversation history available.\")\n",
    "                continue\n",
    "            \n",
    "            elif user_input.lower() == 'clear':\n",
    "                chat_interface.clear_history()\n",
    "                continue\n",
    "            \n",
    "            elif not user_input:\n",
    "                continue\n",
    "            \n",
    "            print(\"\\nü§ñ Assistant: \", end=\"\")\n",
    "            response = chat_interface.chat(user_input)\n",
    "            print(response)\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nüëã Chat interrupted. Goodbye!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Error: {e}\")\n",
    "\n",
    "# Start interactive chat\n",
    "print(\"\"\"\n",
    "üéâ IT Service Management Assistant Setup Complete!\n",
    "\n",
    "üìã SUMMARY:\n",
    "- ‚úÖ AWS Bedrock integration with Claude 3.5 Sonnet\n",
    "- ‚úÖ FAISS vector databases for Change, CMDB, and Incident data\n",
    "- ‚úÖ Self-query retrievers for intelligent information extraction\n",
    "- ‚úÖ LangGraph workflow with router, tools, and judge\n",
    "- ‚úÖ Memory persistence for conversation history\n",
    "- ‚úÖ Multi-tool capability for complex queries\n",
    "\n",
    "üöÄ USAGE:\n",
    "1. Use interactive_chat() for real-time conversation\n",
    "2. Use chat_interface.chat(\"your question\") for single queries\n",
    "3. Use test_chatbot() to run predefined tests\n",
    "\n",
    "üìù AVAILABLE COMMANDS IN INTERACTIVE MODE:\n",
    "- 'history' - View conversation history\n",
    "- 'clear' - Clear conversation history  \n",
    "- 'quit'/'exit'/'bye' - End conversation\n",
    "\n",
    "üîß CUSTOMIZATION:\n",
    "- Replace sample Excel files with your actual data files\n",
    "- Modify the create_vector_store_from_excel() calls with your file paths\n",
    "- Adjust metadata fields and descriptions for your specific use case\n",
    "- Customize the router logic for your specific tool requirements\n",
    "\n",
    "üìä VECTOR STORES LOCATION: ./vector_stores/\n",
    "üíæ CONVERSATION MEMORY: Persisted automatically via MemorySaver\n",
    "\n",
    "Happy chatting! üéØ\n",
    "\"\"\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Ready to start interactive chat!\")\n",
    "print(\"Run: interactive_chat()\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
